{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Requesite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip==20.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=.\n",
    "!cd models/research/ && cp object_detection/packages/tf2/setup.py .\n",
    "!cd models/research/ && python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DATA_FOLDER = '../data/'\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "cloud_class_id = 0\n",
    "water_class_id = 1\n",
    "ground_class_id = 2\n",
    "\n",
    "category_index = {\n",
    "  cloud_class_id: {'id': cloud_class_id, 'name': 'cloud'},\n",
    "  water_class_id: {'id': water_class_id, 'name': 'water'},\n",
    "  ground_class_id: {'id': ground_class_id, 'name': 'ground'},\n",
    "}\n",
    "label_2_category = {'cloud':0,'water':1,'ground':2}\n",
    "# Dimension of former astro pi mission images in data/ folder\n",
    "width, heigth = (2592, 1944)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing XML annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Get Image Annotation from XML Pascal VOC file\n",
    "def get_img_annotations(img_xml_annotations_path):\n",
    "  img_tree = ET.parse(img_xml_annotations_path)\n",
    "  img_root = img_tree.getroot()\n",
    "  labels_list = []\n",
    "  bbox_list = []\n",
    "  for child in img_root:\n",
    "    if child.tag == \"object\":\n",
    "      for element in child:\n",
    "        if element.tag == \"name\":\n",
    "          labels_list.append(element.text)\n",
    "        if element.tag == \"bndbox\":\n",
    "          bbox = []\n",
    "          for coordinates in element:\n",
    "            if coordinates.tag == \"xmin\":\n",
    "              bbox.append(coordinates.text)\n",
    "            if coordinates.tag == \"ymin\":\n",
    "              bbox.append(coordinates.text)\n",
    "            if coordinates.tag == \"xmax\":\n",
    "              bbox.append(coordinates.text)\n",
    "            if coordinates.tag == \"ymax\":\n",
    "              bbox.append(coordinates.text)\n",
    "          bbox_list.append(bbox)\n",
    "  return labels_list, bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Bounding Box so that it is normalized & invariant from image scale dimension\n",
    "def get_prepared_bbox(bbox, width, heigth):\n",
    "  for coord in bbox:\n",
    "    xmin_norm=int(bbox[0])/width\n",
    "    ymin_norm=int(bbox[1])/heigth\n",
    "    xmax_norm=int(bbox[2])/width\n",
    "    ymax_norm=int(bbox[3])/heigth\n",
    "  prepared_bbox = [ymin_norm,xmin_norm,ymax_norm,xmax_norm] # Object Detection Viz utils expected format\n",
    "  return prepared_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepaparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name of astropi Jpeg images\n",
    "def get_images_name(data_folder):\n",
    "  data = os.listdir(data_folder)\n",
    "  images_file_list = [file for file in data if os.path.splitext(file)[1] != '.xml'] # remove xml file\n",
    "  images_file_list.sort()\n",
    "  return images_file_list\n",
    "\n",
    "# Get name of astropi XML annotations\n",
    "def get_bboxes_file_name(data_folder):\n",
    "  data = os.listdir(data_folder)\n",
    "  bbox_file_list = [file for file in data if os.path.splitext(file)[1] != '.jpg']\n",
    "  bbox_file_list.sort()\n",
    "  return bbox_file_list\n",
    "\n",
    "# Get a List of astropi numpy images\n",
    "def get_numpy_images(images_file,data_folder):\n",
    "  np_images_list = []\n",
    "  for image_file in images_file:\n",
    "    img_pil = Image.open(data_folder+image_file)\n",
    "    img_np = asarray(img_pil)\n",
    "    np_images_list.append(img_np)\n",
    "  return np_images_list\n",
    "\n",
    "# Get a List of bboxes & classes for images\n",
    "def get_gt_box_class(bbox_file_list,data_folder):\n",
    "  gt_boxes_list = []\n",
    "  classes_list = []\n",
    "  for bboxes in bbox_file_list: \n",
    "    classes,gt_boxes=get_img_annotations(data_folder+bboxes)\n",
    "    gt_boxes_np = np.array(gt_boxes,dtype=np.float32)\n",
    "    gt_boxes_list.append(gt_boxes_np)\n",
    "    classes_list.append(classes)\n",
    "  return gt_boxes_list, classes_list\n",
    "  \n",
    "# Get a List of class_id for images\n",
    "def get_class_id(classes_list): \n",
    "  classes_id_list = []\n",
    "  for classes in classes_list:\n",
    "    id_list  = []\n",
    "    for label in classes:\n",
    "      id = label_2_category[label]\n",
    "      id_list.append(id)\n",
    "    classes_id_list.append(id_list)\n",
    "  return classes_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Data from raw JPEG & XML files to lists\n",
    "images_list = get_images_name(DATA_FOLDER)\n",
    "np_images_list = get_numpy_images(images_list,DATA_FOLDER)\n",
    "bboxes_file_list = get_bboxes_file_name(DATA_FOLDER)\n",
    "gt_boxes_list, classes_list = get_gt_box_class(bboxes_file_list,DATA_FOLDER)\n",
    "classes_id_list = get_class_id(classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a numpy array of astropi numpy images\n",
    "images_np = np.array(np_images_list) # images as numpy array in case needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Training: convert all numpy arrays & lists to TF tensors\n",
    "\n",
    "def get_tensors_data(np_images_list,gt_boxes_list,classes_id_list):\n",
    "    image_tensors = [] # images tensor list from numpy array\n",
    "    gt_box_tensors = [] # bbox tensor list from gt_box list\n",
    "    class_one_hot_tensors = [] # 1-hot class tensor list from class list\n",
    "\n",
    "    for (image_np, gt_box_np,class_id) in zip(np_images_list, gt_boxes_list,classes_id_list):\n",
    "        image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
    "            image_np, dtype=tf.float32), axis=0))\n",
    "        gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "        indexed_classes = tf.convert_to_tensor(class_id,dtype=tf.int32)\n",
    "        class_one_hot_tensors.append(tf.one_hot(\n",
    "            indexed_classes, num_classes))\n",
    "    print('Done prepping data.')\n",
    "    return image_tensors,gt_box_tensors,class_one_hot_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SSD Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
    "!tar -xf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
    "!if [ -d \"models/research/object_detection/test_data/checkpoint\" ]; then rm -Rf models/research/object_detection/test_data/checkpoint; fi\n",
    "!mkdir models/research/object_detection/test_data/checkpoint\n",
    "!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Pipeline & Checkpoint config\n",
    "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
    "num_classes = 3\n",
    "pipeline_config = './models/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config'\n",
    "checkpoint_path = './models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "# This will be where we save checkpoint & config for TFLite conversion later.\n",
    "output_directory = './output/'\n",
    "output_checkpoint_dir = os.path.join(output_directory, 'checkpoint')\n",
    "\n",
    "# Load pipeline config and build a detection model.\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes # to set number of classes to 3\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=True) # the actual SSD Mobilenet Detection model\n",
    "      \n",
    "# Save new pipeline config\n",
    "pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_proto, output_directory)\n",
    "\n",
    "# To save checkpoint for TFLite conversion.\n",
    "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    exported_ckpt, output_checkpoint_dir, max_to_keep=1)\n",
    "\n",
    "# Run model through a dummy image so that variables are created\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 320, 320, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "print('Weights restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "image_tensors,gt_box_tensors,class_one_hot_tensors = get_tensors_data(np_images_list,gt_boxes_list,classes_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.15\n",
    "num_batches = 1\n",
    "\n",
    "# Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "for var in trainable_variables:\n",
    "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "    to_fine_tune.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up forward + backward pass for a single train step.\n",
    "\n",
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "  \"\"\"Get a tf.function for training step.\"\"\"\n",
    "\n",
    "  # Use tf.function for a bit of speed.\n",
    "  # Comment out the tf.function decorator if you want the inside of the\n",
    "  # function to run eagerly.\n",
    "  @tf.function\n",
    "  def train_step_fn(image_tensors,\n",
    "                    groundtruth_boxes_list,\n",
    "                    groundtruth_classes_list):\n",
    "    shapes = tf.constant(batch_size * [[320, 320, 3]], dtype=tf.int32)\n",
    "    model.provide_groundtruth(\n",
    "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "        groundtruth_classes_list=groundtruth_classes_list)\n",
    "    with tf.GradientTape() as tape:\n",
    "      preprocessed_images = tf.concat(\n",
    "          [detection_model.preprocess(image_tensor)[0]\n",
    "           for image_tensor in image_tensors], axis=0)\n",
    "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "      losses_dict = model.loss(prediction_dict, shapes)\n",
    "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "    return total_loss\n",
    "\n",
    "  return train_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "train_step_fn = get_model_train_step_function(\n",
    "    detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "# Training using train_step function\n",
    "print('Start fine-tuning Training!', flush=True)\n",
    "for idx in range(num_batches):\n",
    "  # Grab keys for a random subset of examples\n",
    "  all_keys = list(range(len(np_images_list)))\n",
    "  random.shuffle(all_keys)\n",
    "  example_keys = all_keys[:batch_size]\n",
    "\n",
    "  # Instantiate gt_boxes, gt_classes, images \n",
    "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "  gt_classes_list = [class_one_hot_tensors[key] for key in example_keys]\n",
    "  image_tensors = [image_tensors[key] for key in example_keys]\n",
    "\n",
    "  # Training step (forward pass + backwards pass)\n",
    "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "\n",
    "  if idx % 2 == 0:\n",
    "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
    "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
    "\n",
    "print('Done fine-tuning!')\n",
    "\n",
    "ckpt_manager.save()\n",
    "print('Checkpoint saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model as savedModel into export folder with exporter_main_v2.py\n",
    "!python models/research/object_detection/exporter_main_v2.py \\\n",
    "--pipeline_config_path output/pipeline.config \\\n",
    "--trained_checkpoint_dir output/checkpoint \\\n",
    "--output_directory export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFlite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/research/object_detection/export_tflite_graph_tf2.py \\\n",
    "  --pipeline_config_path output/pipeline.config \\\n",
    "  --trained_checkpoint_dir export/checkpoint \\\n",
    "  --output_directory tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5795bd8c0054127f1bb8e33543dcf146a0412f86b0576855391447d0232ac43b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bluelens_train': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
